{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snitter og lagrer alle embeddingsene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snitt(embeddings):\n",
    "  snitt = [emb.mean(dim=0).numpy() for emb in embeddings]\n",
    "  return snitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snitt2(embeddings):\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n",
    "    snitt2 = torch.stack(tensor_embeddings).mean(dim=0)\n",
    "    return snitt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ã…rsrapporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exxon\n",
    "\n",
    "with open(\"data/Exxon_pdfs.pkl\", \"rb\") as file:\n",
    "    exxon_annual = pickle.load(file)\n",
    "\n",
    "exxon_annual = snitt(list(exxon_annual.values()))\n",
    "print(f\"Exxon pdfs ({len(exxon_annual)}): {exxon_annual}\")  ## 2005-2023\n",
    "\n",
    "with open(\"exxon_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(exxon_annual,file)\n",
    "\n",
    "print(f\"Exxon pdfs ({len(exxon_annual)}): {exxon_annual}\")  ## 2005-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['2023', '2022', '2021', '2020', '2019', '2018', '2017', '2016', '2015', '2014', '2013', '2012', '2011', '2010', '2009', '2008', '2007', '2006'])\n",
      "TotalEnergies annual (18): [tensor([-0.0805,  0.5748,  0.0537,  ..., -0.5654,  0.0386,  0.1145]), tensor([-0.1245,  0.4820,  0.0875,  ..., -0.5067,  0.0260,  0.0446]), tensor([-0.1247,  0.5291,  0.0474,  ..., -0.5077, -0.0198,  0.1180]), tensor([-0.0928,  0.5179,  0.0333,  ..., -0.5023, -0.0854,  0.1075]), tensor([-0.0957, -0.6846, -0.0909,  ...,  0.0758, -0.1351, -0.0555]), tensor([-0.0980, -0.6837, -0.0977,  ...,  0.0771, -0.1374, -0.0559]), tensor([-0.1035, -0.6897, -0.0948,  ...,  0.0747, -0.1378, -0.0541]), tensor([-0.0985, -0.6863, -0.0904,  ...,  0.0773, -0.1358, -0.0558]), tensor([-0.1001, -0.6812, -0.0886,  ...,  0.0754, -0.1362, -0.0523]), tensor([-0.0962, -0.6863, -0.0904,  ...,  0.0698, -0.1389, -0.0529]), tensor([-0.0988, -0.6935, -0.1059,  ...,  0.0806, -0.1365, -0.0457]), tensor([-0.2602,  0.4919, -0.1675,  ..., -0.4459,  0.0448,  0.1436]), tensor([-0.1031, -0.6671, -0.1342,  ...,  0.0303, -0.1158, -0.0339]), tensor([-0.3035,  0.3979, -0.1686,  ..., -0.4692, -0.0320,  0.1014]), tensor([-0.2963,  0.4559, -0.1402,  ..., -0.5181,  0.0902,  0.0831]), tensor([-0.3209,  0.6182, -0.1313,  ..., -0.3984,  0.0248,  0.0249]), tensor([-0.3067,  0.6398, -0.0965,  ..., -0.4732,  0.0415,  0.0177]), tensor([-0.2786,  0.6229, -0.1587,  ..., -0.4502,  0.0239,  0.0501])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## TotalEnergies\n",
    "\n",
    "with open(\"data/annual_totalenergies.pkl\", \"rb\") as file:\n",
    "    totalEnergies_pdfs = pickle.load(file)\n",
    "\n",
    "print(totalEnergies_pdfs.keys())\n",
    "\n",
    "totalEnergies_annual = []\n",
    "for year in totalEnergies_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in totalEnergies_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    totalEnergies_annual.append(mean)\n",
    "totalEnergies_annual.reverse()\n",
    "print(f\"TotalEnergies annual ({len(totalEnergies_annual)}): {totalEnergies_annual}\")  ## 2006-2023\n",
    "\n",
    "with open(\"totalEnergies_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(totalEnergies_annual,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aker BP pdfs (15): [tensor([ 0.4496,  0.7144,  0.0890,  ..., -1.0203, -0.5103,  0.2943]), tensor([ 0.3717,  0.5783,  0.1684,  ..., -0.9368, -0.4056,  0.2735]), tensor([ 0.4339,  0.4672,  0.0503,  ..., -0.7920, -0.3806,  0.2254]), tensor([-0.0130,  0.3270,  0.0514,  ..., -0.7376, -0.2469,  0.2719]), tensor([ 0.0053,  0.3280,  0.0918,  ..., -0.6227, -0.2649,  0.2980]), tensor([ 0.0400,  0.3700,  0.1783,  ..., -0.6663, -0.3817,  0.2949]), tensor([ 0.1662,  0.2601, -0.0888,  ..., -0.6957, -0.6634,  0.4423]), tensor([ 0.1434,  0.0996,  0.3218,  ..., -0.6920, -0.6728,  0.0150]), tensor([ 0.0056,  0.3316,  0.0505,  ..., -0.8696, -0.5513,  0.1292]), tensor([ 0.0896,  0.2809,  0.0614,  ..., -0.9446, -0.6048,  0.1295]), tensor([ 0.0772,  0.2754,  0.0418,  ..., -0.8781, -0.6591,  0.1122]), tensor([ 0.1263,  0.2631, -0.0113,  ..., -0.8838, -0.6215,  0.0877]), tensor([ 0.0964,  0.2723, -0.0264,  ..., -0.8215, -0.5835,  0.0910]), tensor([ 0.0360,  0.2449,  0.0097,  ..., -0.7858, -0.5758,  0.0709]), tensor([ 0.0680,  0.2795, -0.0952,  ..., -0.8255, -0.5588,  0.1296])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## AkerBP\n",
    "\n",
    "with open(\"data/annual_AkerBP.pkl\", \"rb\") as file:\n",
    "    akerBP_pdfs = pickle.load(file)\n",
    "\n",
    "akerBP_annual = []\n",
    "for year in akerBP_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in akerBP_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    akerBP_annual.append(mean)\n",
    "akerBP_annual.reverse()\n",
    "\n",
    "print(f\"Aker BP pdfs ({len(akerBP_annual)}): {akerBP_annual}\")  ## 2009-2023\n",
    "\n",
    "with open(\"akerBP_annual_snitt\",\"wb\") as file:\n",
    "    pickle.dump(akerBP_annual,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chevron pdfs (29): [tensor([-0.0661,  0.5594,  0.0784,  ..., -0.5134, -0.1062, -0.0410]), tensor([-0.1205,  0.5682,  0.1302,  ..., -0.5190, -0.1122, -0.0283]), tensor([-0.1422,  0.6586,  0.0729,  ..., -0.5296, -0.1574, -0.0380]), tensor([-0.1394,  0.5013, -0.0505,  ..., -0.5059, -0.1669,  0.0253]), tensor([-0.1554,  0.3678,  0.1085,  ..., -0.5528, -0.1309, -0.0345]), tensor([-0.0582,  0.3542,  0.1054,  ..., -0.6133, -0.1051, -0.0363]), tensor([-0.0438,  0.2954, -0.0657,  ..., -0.5288, -0.1427, -0.0673]), tensor([-0.1720,  0.5488, -0.2328,  ..., -0.5560, -0.0726,  0.1061]), tensor([-0.1339,  0.5308, -0.2633,  ..., -0.6178, -0.0596,  0.1203]), tensor([-0.1397,  0.5380, -0.2469,  ..., -0.6136, -0.0450,  0.1399]), tensor([-0.1560,  0.5473, -0.1461,  ..., -0.5115, -0.0482,  0.0715]), tensor([-0.1824,  0.5203, -0.0884,  ..., -0.5387, -0.0199,  0.0507]), tensor([-0.2451,  0.5275, -0.0481,  ..., -0.5563,  0.0287,  0.0457]), tensor([-0.2393,  0.5199, -0.0915,  ..., -0.5196,  0.0109,  0.0987]), tensor([-0.2296,  0.5412, -0.0430,  ..., -0.4593, -0.0453,  0.0911]), tensor([-0.2092,  0.5362, -0.1759,  ..., -0.4932, -0.0120,  0.1075]), tensor([-0.1842,  0.5057, -0.2461,  ..., -0.3950, -0.0020,  0.0670]), tensor([-0.3084,  0.4589, -0.1882,  ..., -0.4901,  0.0577,  0.0952]), tensor([-0.2984,  0.3985, -0.1140,  ..., -0.3634,  0.0139,  0.0922]), tensor([-0.2669,  0.4975, -0.0930,  ..., -0.3991, -0.0470,  0.1309]), tensor([-0.2262,  0.4799, -0.0645,  ..., -0.4292, -0.0774,  0.1319]), tensor([-0.2962,  0.4597, -0.0919,  ..., -0.3769, -0.0933,  0.0886]), tensor([-0.0996, -0.5719, -0.1374,  ...,  0.0132, -0.1076, -0.0206]), tensor([-0.0848, -0.4198, -0.1102,  ..., -0.1720, -0.1050, -0.0365]), tensor([-0.0981, -0.4683, -0.0996,  ..., -0.1199, -0.1041, -0.0329]), tensor([-0.0602, -0.3473, -0.1587,  ..., -0.1835, -0.0650, -0.0151]), tensor([-0.0945, -0.4434, -0.1093,  ..., -0.1298, -0.1153, -0.0347]), tensor([-0.1288,  0.2273, -0.0035,  ..., -0.4934, -0.1012, -0.0966]), tensor([-0.1316,  0.2174,  0.0349,  ..., -0.4711, -0.1159, -0.0983])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## Chevron\n",
    "\n",
    "with open(\"data/Chevron_annual.pkl\",\"rb\") as file: \n",
    "    chevron_pdfs = pickle.load(file)\n",
    "\n",
    "chevron_annual = []\n",
    "for year in chevron_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in chevron_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    chevron_annual.append(mean)\n",
    "\n",
    "print(f\"Chevron pdfs ({len(chevron_annual)}): {chevron_annual}\")  ## 1995-2023\n",
    "\n",
    "with open(\"chevron_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(chevron_annual,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BP pdfs (14): [tensor([-0.0847,  0.3093, -0.1967,  ..., -0.4559, -0.0487,  0.1038]), tensor([-0.0327,  0.3027, -0.3805,  ..., -0.5664, -0.0235,  0.1981]), tensor([-0.1290,  0.3374, -0.3669,  ..., -0.5968, -0.0014,  0.2097]), tensor([-0.1340,  0.3590, -0.2757,  ..., -0.4754,  0.0047,  0.1799]), tensor([-0.1422,  0.3915, -0.2400,  ..., -0.4841, -0.0427,  0.1745]), tensor([-0.1036,  0.3838, -0.2771,  ..., -0.5005,  0.0122,  0.1665]), tensor([-0.1409,  0.4662, -0.3501,  ..., -0.4992,  0.0075,  0.2049]), tensor([-0.1748,  0.4649, -0.3080,  ..., -0.5019, -0.0017,  0.1444]), tensor([-0.2132,  0.5249, -0.2783,  ..., -0.5852, -0.0507,  0.1335]), tensor([-0.1433,  0.4537, -0.2642,  ..., -0.4974, -0.0895,  0.1029]), tensor([-0.0920,  0.4211, -0.3082,  ..., -0.5695,  0.0601,  0.1546]), tensor([-0.1389,  0.4664, -0.3580,  ..., -0.5551,  0.0081,  0.0585]), tensor([-0.1714,  0.4737, -0.3313,  ..., -0.5683, -0.0408,  0.0262]), tensor([-0.1037,  0.4496, -0.2654,  ..., -0.5351, -0.0613,  0.0467])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## BP\n",
    "\n",
    "with open(\"data/BP_annual.pkl\",\"rb\") as file: \n",
    "    BP_pdfs = pickle.load(file)\n",
    "\n",
    "BP_annual = []\n",
    "for year in BP_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in BP_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    BP_annual.append(mean)\n",
    "BP_annual.reverse()\n",
    "\n",
    "print(f\"BP pdfs ({len(BP_annual)}): {BP_annual}\")  ## 2010-2023\n",
    "\n",
    "with open(\"BP_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(BP_annual,file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023'])\n",
      "PetroChina pdfs (8): [tensor([-0.2083,  0.4838, -0.1474,  ..., -0.7009, -0.0333,  0.2400]), tensor([-0.4084,  0.3596,  0.0521,  ..., -0.6551, -0.2543, -0.0885]), tensor([-0.3978,  0.3219,  0.0559,  ..., -0.6697, -0.3419, -0.0485]), tensor([-0.4384,  0.2202,  0.0331,  ..., -0.6693, -0.3840, -0.0593]), tensor([-0.3721,  0.2673, -0.0374,  ..., -0.6288, -0.3533, -0.0501]), tensor([-0.3476,  0.3528, -0.0742,  ..., -0.6549, -0.2783, -0.0427]), tensor([-0.3964,  0.2561,  0.0527,  ..., -0.6965, -0.4007, -0.0736]), tensor([-0.2849,  0.3116,  0.0181,  ..., -0.6572, -0.3741, -0.0310])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## PetroChina\n",
    "\n",
    "with open(\"data/PetroChina_annual.pkl\",\"rb\") as file: \n",
    "    PetroChina_pdfs = pickle.load(file)\n",
    "\n",
    "print(PetroChina_pdfs.keys())\n",
    "\n",
    "PetroChina_annual = []\n",
    "for year in PetroChina_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in PetroChina_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    PetroChina_annual.append(mean)\n",
    "#PetroChina_annual.reverse()\n",
    "\n",
    "print(f\"PetroChina pdfs ({len(PetroChina_annual)}): {PetroChina_annual}\")  ## 2016-2023\n",
    "\n",
    "with open(\"petroChina_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(PetroChina_annual,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valero pdfs (21): [tensor([-0.1791,  0.3376,  0.0290,  ..., -0.3271, -0.1223,  0.0798]), tensor([-0.1810,  0.4418,  0.0362,  ..., -0.2865, -0.0862,  0.1513]), tensor([-0.0763, -0.1401, -0.1048,  ..., -0.1077, -0.1197,  0.0384]), tensor([-0.0260,  0.3169, -0.0264,  ..., -0.2818, -0.0259,  0.1241]), tensor([-0.0993,  0.4097, -0.0571,  ..., -0.3552, -0.0607,  0.1077]), tensor([-0.1119,  0.3679, -0.1460,  ..., -0.2845, -0.0095,  0.1159]), tensor([-0.1348,  0.3816, -0.1538,  ..., -0.3803, -0.0787,  0.0794]), tensor([-0.0766,  0.4424, -0.3135,  ..., -0.3433, -0.0526,  0.2025]), tensor([-0.1969,  0.4438, -0.3200,  ..., -0.2285,  0.0334,  0.1706]), tensor([-0.3191,  0.3194, -0.0389,  ..., -0.2488,  0.0026,  0.0632]), tensor([-0.3634,  0.3339,  0.0261,  ..., -0.3291,  0.0075,  0.1098]), tensor([-0.3339,  0.3011,  0.0232,  ..., -0.2547,  0.0417,  0.1008]), tensor([-0.3235,  0.3104, -0.0094,  ..., -0.1993,  0.0338,  0.0578]), tensor([-0.3020,  0.2698, -0.0146,  ..., -0.2287,  0.0180,  0.0469]), tensor([-0.3304,  0.2620, -0.0672,  ..., -0.2361,  0.0166,  0.0469]), tensor([-0.3291,  0.3154, -0.0360,  ..., -0.2582,  0.0009,  0.0438]), tensor([-0.3265,  0.2450, -0.0692,  ..., -0.2954, -0.0238,  0.0358]), tensor([-0.3504,  0.2141, -0.1033,  ..., -0.3079, -0.0340,  0.0269]), tensor([-0.3514,  0.2458, -0.1619,  ..., -0.3174,  0.0214,  0.0147]), tensor([-0.3153,  0.3161, -0.1260,  ..., -0.2940,  0.0893,  0.0161]), tensor([-0.3162,  0.3737, -0.1402,  ..., -0.2712,  0.0866, -0.0084])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## Valero\n",
    "\n",
    "with open(\"data/Valero_annual.pkl\",\"rb\") as file: \n",
    "    Valero_pdfs = pickle.load(file)\n",
    "\n",
    "Valero_annual = []\n",
    "for year in Valero_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in Valero_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    Valero_annual.append(mean)\n",
    "#Valero_annual.reverse()\n",
    "\n",
    "print(f\"Valero pdfs ({len(Valero_annual)}): {Valero_annual}\")  ## 2002-2022\n",
    "\n",
    "with open(\"valero_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(Valero_annual,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1985', '1986', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023'])\n",
      "RelianceIndustries pdfs (36): [tensor([-0.4073,  0.0853,  0.0329,  ..., -0.2900, -0.0300, -0.1771]), tensor([-0.4348,  0.0224,  0.0273,  ..., -0.3317, -0.0879, -0.1679]), tensor([-4.1258e-01,  9.6817e-02,  2.5947e-02,  ..., -2.5862e-01,\n",
      "         2.8300e-06, -1.0481e-01]), tensor([-0.4244,  0.1082, -0.0070,  ..., -0.3210, -0.0522, -0.1892]), tensor([-0.3394,  0.1705,  0.0061,  ..., -0.3429, -0.0965, -0.1757]), tensor([-0.2384,  0.0926,  0.0186,  ..., -0.3862, -0.0800, -0.1956]), tensor([-0.2362,  0.2312, -0.0293,  ..., -0.2850, -0.0328, -0.1859]), tensor([-0.3905,  0.3235, -0.0811,  ..., -0.2829,  0.0288, -0.2147]), tensor([-0.4935,  0.3022, -0.1193,  ..., -0.3349,  0.0738, -0.1551]), tensor([-0.5953,  0.4729, -0.0973,  ..., -0.3785,  0.0219, -0.2487]), tensor([-0.4317,  0.4467, -0.1696,  ..., -0.3116,  0.0188, -0.1160]), tensor([-0.4382,  0.3115, -0.1744,  ..., -0.2918, -0.0875, -0.1579]), tensor([-0.2972,  0.1021, -0.1169,  ..., -0.4387, -0.1125, -0.2578]), tensor([-0.2843,  0.2493, -0.0483,  ..., -0.2923, -0.0355, -0.1516]), tensor([-0.3778,  0.2132, -0.1722,  ..., -0.3132,  0.0494, -0.1365]), tensor([-0.3602,  0.1820, -0.1578,  ..., -0.4498,  0.0318, -0.1509]), tensor([-0.4087,  0.3101, -0.1639,  ..., -0.3564, -0.0072, -0.1112]), tensor([-3.6429e-01,  2.0115e-01, -1.4877e-01,  ..., -4.1461e-01,\n",
      "         2.2664e-04, -1.6570e-01]), tensor([-0.2642,  0.2277, -0.0661,  ..., -0.4270,  0.0497, -0.1137]), tensor([-0.2936,  0.3159, -0.1614,  ..., -0.5134, -0.0363, -0.0820]), tensor([-0.3595,  0.3119, -0.1477,  ..., -0.4894, -0.0098, -0.1275]), tensor([-0.3360,  0.3158, -0.1289,  ..., -0.4265,  0.0085, -0.1158]), tensor([-0.2892,  0.3719, -0.0801,  ..., -0.4016, -0.0374, -0.0613]), tensor([-0.1494, -0.2163,  0.0395,  ..., -0.4266, -0.1752, -0.2036]), tensor([-0.6244,  0.3251, -0.4672,  ..., -0.4174,  0.3215, -0.2015]), tensor([-0.4368,  0.2863, -0.6246,  ..., -0.5347,  0.0910,  0.1771]), tensor([-0.3138,  0.0423, -0.1671,  ..., -0.4197, -0.0584, -0.0772]), tensor([-0.2920,  0.1784, -0.3502,  ..., -0.4367,  0.0438,  0.0536]), tensor([-0.3210,  0.1220, -0.2812,  ..., -0.3311,  0.0271,  0.0705]), tensor([-0.3948,  0.2109, -0.2710,  ..., -0.3484,  0.0212,  0.0653]), tensor([-0.3158,  0.2475, -0.3414,  ..., -0.4497,  0.0485,  0.0760]), tensor([-0.4042,  0.1840, -0.3448,  ..., -0.3826, -0.0294,  0.1229]), tensor([-0.4110,  0.2116, -0.3810,  ..., -0.3671, -0.0778,  0.0357]), tensor([-0.3486,  0.3108, -0.5039,  ..., -0.4243,  0.0033,  0.0725]), tensor([-0.3842,  0.3037, -0.3856,  ..., -0.3756,  0.0189,  0.0671]), tensor([-0.3651,  0.2886, -0.4334,  ..., -0.3945,  0.0213,  0.1885])]\n",
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## RelianceIndustries\n",
    "\n",
    "with open(\"data/RelianceIndustries_annual.pkl\",\"rb\") as file: \n",
    "    RelianceIndustries_pdfs = pickle.load(file)\n",
    "\n",
    "print(RelianceIndustries_pdfs.keys())\n",
    "\n",
    "RelianceIndustries_annual = []\n",
    "for year in RelianceIndustries_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in RelianceIndustries_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    RelianceIndustries_annual.append(mean)\n",
    "RelianceIndustries_annual = RelianceIndustries_annual[2:]\n",
    "\n",
    "print(f\"RelianceIndustries pdfs ({len(RelianceIndustries_annual)}): {RelianceIndustries_annual}\")  ## 1988-2023\n",
    "\n",
    "with open(\"reliance_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(RelianceIndustries_annual,file)\n",
    "\n",
    "print(len(RelianceIndustries_annual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023'])\n",
      "CNOOC pdfs (9): [tensor([-0.1767, -0.0087, -0.0069,  ..., -0.5226, -0.1660,  0.0119]), tensor([-0.1891, -0.0241, -0.0273,  ..., -0.5392, -0.1926, -0.0096]), tensor([-1.8506e-01,  1.8037e-02, -5.1190e-02,  ..., -5.7887e-01,\n",
      "        -1.9730e-01,  5.4809e-04]), tensor([-0.2110,  0.0037, -0.0475,  ..., -0.5902, -0.1994, -0.0114]), tensor([-0.2130, -0.0239, -0.0147,  ..., -0.5866, -0.2369, -0.0328]), tensor([-0.2125,  0.0079, -0.0622,  ..., -0.6238, -0.2165, -0.0204]), tensor([-0.2474,  0.0441, -0.1005,  ..., -0.5959, -0.1953,  0.0158]), tensor([-0.2469,  0.0239, -0.0445,  ..., -0.6181, -0.2248, -0.0105]), tensor([-0.2424,  0.0490, -0.0653,  ..., -0.6107, -0.2224,  0.0025])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## CNOOC\n",
    "\n",
    "with open(\"data/CNOOC_annual2.pkl\",\"rb\") as file: \n",
    "    CNOOC_pdfs = pickle.load(file)\n",
    "\n",
    "print(CNOOC_pdfs.keys())\n",
    "\n",
    "CNOOC_annual = []\n",
    "for year in CNOOC_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in CNOOC_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    CNOOC_annual.append(mean)\n",
    "\n",
    "print(f\"CNOOC pdfs ({len(CNOOC_annual)}): {CNOOC_annual}\")  ## 2015-2023\n",
    "\n",
    "with open(\"CNOOC_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(CNOOC_annual,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023'])\n",
      "conocophillips pdfs (19): [tensor([-0.0178,  0.6208, -0.1759,  ..., -0.5987,  0.0554,  0.1364]), tensor([-0.0626,  0.4722, -0.1257,  ..., -0.5783,  0.1185,  0.1173]), tensor([-0.0944, -0.6716, -0.1207,  ...,  0.0484, -0.1290, -0.0287]), tensor([-0.0987, -0.5162, -0.1495,  ..., -0.0210, -0.0944,  0.0165]), tensor([ 0.0364,  0.5698, -0.3936,  ..., -0.6724,  0.1064,  0.1341]), tensor([-0.2809,  0.2786,  0.0327,  ..., -0.2289, -0.0040,  0.0636]), tensor([-0.0409,  0.6014, -0.4810,  ..., -0.6810,  0.0941,  0.2028]), tensor([ 0.0405,  0.4907, -0.5094,  ..., -0.6733,  0.1089,  0.1696]), tensor([ 0.0826,  0.4211, -0.4004,  ..., -0.6799, -0.0171,  0.2715]), tensor([ 0.0523,  0.2938, -0.2745,  ..., -0.5812, -0.0466,  0.1416]), tensor([ 0.0688,  0.2765, -0.1981,  ..., -0.5571, -0.1097,  0.0437]), tensor([-0.0232,  0.4328, -0.2052,  ..., -0.4818, -0.1478,  0.0579]), tensor([-0.0144,  0.4683, -0.2122,  ..., -0.5458, -0.1053,  0.0340]), tensor([-0.0320,  0.4125, -0.1708,  ..., -0.5474, -0.1773, -0.0174]), tensor([-0.0665,  0.3853, -0.1787,  ..., -0.5472, -0.1791,  0.0276]), tensor([-0.0231,  0.2827, -0.2589,  ..., -0.5839, -0.1779, -0.0048]), tensor([-0.0269,  0.5738, -0.3424,  ..., -0.7833, -0.0577,  0.0483]), tensor([-0.0091,  0.5228, -0.2452,  ..., -0.7629, -0.0594,  0.0428]), tensor([ 0.1429,  0.3127, -0.2493,  ..., -0.4899, -0.3449,  0.0569])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1809046617.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in conocophillips_pdfs[year]]\n",
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## ConocoPhillips\n",
    "\n",
    "with open(\"data/conocophillips_annual.pkl\",\"rb\") as file: \n",
    "    conocophillips_pdfs = pickle.load(file)\n",
    "\n",
    "print(conocophillips_pdfs.keys())\n",
    "\n",
    "conocophillips_annual = []\n",
    "for year in conocophillips_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in conocophillips_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    conocophillips_annual.append(mean)\n",
    "\n",
    "print(f\"conocophillips pdfs ({len(conocophillips_annual)}): {conocophillips_annual}\")  ## 2005-2023\n",
    "\n",
    "with open(\"conocophillips_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(conocophillips_annual,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023'])\n",
      "Enbridge pdfs (13): [tensor([-0.2536,  0.4414, -0.4092,  ..., -0.4111,  0.2736,  0.0661]), tensor([-0.2657,  0.4615, -0.3113,  ..., -0.6026,  0.1136,  0.0685]), tensor([-0.2119,  0.4179, -0.3716,  ..., -0.6040,  0.0864,  0.1022]), tensor([-0.1579,  0.5145, -0.3548,  ..., -0.6024,  0.0947,  0.1046]), tensor([-0.1450,  0.4551, -0.3109,  ..., -0.5717,  0.0705,  0.1107]), tensor([-0.2172,  0.4758, -0.3130,  ..., -0.5514,  0.0291,  0.0898]), tensor([-0.1860,  0.3829, -0.2544,  ..., -0.4889,  0.0705,  0.0915]), tensor([-0.1932,  0.3274, -0.2478,  ..., -0.4675,  0.0080,  0.0680]), tensor([-0.2081,  0.3250, -0.2770,  ..., -0.5076, -0.0232,  0.0265]), tensor([-0.1976,  0.3657, -0.3489,  ..., -0.4800,  0.0016,  0.0354]), tensor([-0.2106,  0.3759, -0.3418,  ..., -0.4835,  0.0160,  0.0271]), tensor([-0.2303,  0.3469, -0.2856,  ..., -0.4998, -0.0145,  0.0141]), tensor([-0.2256,  0.3594, -0.2868,  ..., -0.4879, -0.0054,  0.0201])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/2016254701.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in enbridge_pdfs[year]]\n",
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## Enbridge\n",
    "\n",
    "with open(\"data/enbridge_annual.pkl\",\"rb\") as file: \n",
    "    enbridge_pdfs = pickle.load(file)\n",
    "\n",
    "print(enbridge_pdfs.keys())\n",
    "\n",
    "enbridge_annual = []\n",
    "for year in enbridge_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in enbridge_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    enbridge_annual.append(mean)\n",
    "\n",
    "print(f\"Enbridge pdfs ({len(enbridge_annual)}): {enbridge_annual}\")  ## 2011-2023\n",
    "\n",
    "with open(\"enbridge_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(enbridge_annual,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023'])\n",
      "EOG pdfs (25): [tensor([-0.1188,  0.3368, -0.0614,  ..., -0.4735, -0.0799, -0.0569]), tensor([-0.0365,  0.3421, -0.1620,  ..., -0.5862, -0.0616, -0.0268]), tensor([-0.0924,  0.2844, -0.1883,  ..., -0.4481, -0.0357, -0.0312]), tensor([-0.0817, -0.6290, -0.1342,  ...,  0.0252, -0.1387, -0.0467]), tensor([-8.8970e-02, -6.0397e-01, -1.4668e-01,  ..., -2.2017e-04,\n",
      "        -1.3076e-01, -3.2832e-02]), tensor([-0.0889, -0.5742, -0.1446,  ..., -0.0184, -0.1245, -0.0338]), tensor([-0.0759, -0.6689, -0.1503,  ...,  0.0342, -0.1288, -0.0336]), tensor([-0.0793, -0.6521, -0.1484,  ...,  0.0263, -0.1187, -0.0306]), tensor([-0.0693, -0.3020, -0.1503,  ..., -0.2124, -0.0616,  0.0203]), tensor([-0.0669, -0.1260, -0.1764,  ..., -0.2720, -0.0815,  0.0428]), tensor([-0.0743,  0.4749, -0.1938,  ..., -0.5931, -0.0688,  0.1254]), tensor([-0.0576, -0.0899, -0.2023,  ..., -0.3147, -0.0613,  0.0339]), tensor([-0.0614, -0.3018, -0.1894,  ..., -0.1994, -0.0824,  0.0176]), tensor([-0.1018, -0.3038, -0.1574,  ..., -0.2044, -0.0635,  0.0328]), tensor([-0.0887, -0.3251, -0.1577,  ..., -0.1727, -0.0763,  0.0397]), tensor([-0.0729, -0.3881, -0.1658,  ..., -0.1370, -0.1056,  0.0178]), tensor([-0.0807,  0.2920, -0.1728,  ..., -0.4905, -0.0377,  0.1318]), tensor([-0.0867,  0.3383, -0.1962,  ..., -0.5251, -0.0777,  0.1138]), tensor([-0.1094,  0.3439, -0.2062,  ..., -0.5373, -0.0772,  0.0899]), tensor([-0.1193,  0.3389, -0.2132,  ..., -0.5538, -0.0915,  0.0939]), tensor([-0.1019,  0.3331, -0.2332,  ..., -0.5814, -0.0977,  0.0829]), tensor([-0.0474,  0.2750, -0.2133,  ..., -0.5837, -0.1464,  0.0404]), tensor([-0.0599,  0.3093, -0.2623,  ..., -0.5338, -0.1230,  0.0511]), tensor([-0.0555,  0.3241, -0.2212,  ..., -0.5480, -0.1357,  0.0283]), tensor([-0.0488,  0.3293, -0.2208,  ..., -0.5298, -0.1239,  0.0392])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/710228566.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in eog_resources_pdfs[year]]\n",
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## EOG\n",
    "\n",
    "with open(\"data/eog_resources.pkl\",\"rb\") as file: \n",
    "    eog_resources_pdfs = pickle.load(file)\n",
    "    \n",
    "print(eog_resources_pdfs.keys())\n",
    "\n",
    "eog_resources_annual = []\n",
    "for year in eog_resources_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in eog_resources_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    eog_resources_annual.append(mean)\n",
    "\n",
    "print(f\"EOG pdfs ({len(eog_resources_annual)}): {eog_resources_annual}\")  ## 1999-2023\n",
    "\n",
    "with open(\"eog_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(eog_resources_annual,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023'])\n",
      "Marathon pdfs (13): [tensor([-0.1767,  0.2324, -0.1613,  ..., -0.3592,  0.0394, -0.0043]), tensor([-0.2364,  0.2351, -0.0738,  ..., -0.3719,  0.0466,  0.0262]), tensor([-0.2163,  0.2244, -0.0937,  ..., -0.3422,  0.0192,  0.0297]), tensor([-0.2147,  0.2712, -0.0646,  ..., -0.3484,  0.0054,  0.0151]), tensor([-0.1551,  0.2459, -0.0740,  ..., -0.4082, -0.0197,  0.0310]), tensor([-0.1647,  0.2671, -0.0828,  ..., -0.3977, -0.0119,  0.0302]), tensor([-0.1891,  0.2550, -0.0824,  ..., -0.3876, -0.0204, -0.0026]), tensor([-0.1804,  0.1660, -0.0800,  ..., -0.3850, -0.0346, -0.0461]), tensor([-0.1957,  0.1990, -0.1009,  ..., -0.3826, -0.0510, -0.0382]), tensor([-0.2353,  0.2410, -0.1336,  ..., -0.3811, -0.0147, -0.0047]), tensor([-0.2234,  0.2696, -0.1650,  ..., -0.3784, -0.0208,  0.0062]), tensor([-0.2053,  0.2896, -0.1808,  ..., -0.4343, -0.0104, -0.0010]), tensor([-0.1824,  0.2694, -0.1986,  ..., -0.4177, -0.0144,  0.0099])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/366872068.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in marathon_pdfs[year]]\n",
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## Marathon\n",
    "\n",
    "with open(\"data/marathon_annual.pkl\",\"rb\") as file: \n",
    "    marathon_pdfs = pickle.load(file)\n",
    "\n",
    "print(marathon_pdfs.keys())\n",
    "\n",
    "marathon_annual = []\n",
    "for year in marathon_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in marathon_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    marathon_annual.append(mean)\n",
    "\n",
    "print(f\"Marathon pdfs ({len(marathon_annual)}): {marathon_annual}\")  ## 2011-2023\n",
    "\n",
    "with open(\"marathon_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(marathon_annual,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([2023, 2022, 2021, 2020, 2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008, 2007, 2006, 2005, 2004, 2003, 2002, 2001, 2000, 1999])\n",
      "Equinor pdfs (25): [tensor([-0.1062,  0.4533, -0.1082,  ..., -0.6821, -0.0672,  0.2173]), tensor([-0.1138,  0.5336, -0.1884,  ..., -0.5507, -0.0912,  0.1468]), tensor([-0.1274,  0.4955, -0.1362,  ..., -0.6382, -0.1255,  0.2079]), tensor([-0.1606,  0.4818, -0.1941,  ..., -0.6436, -0.0498,  0.2228]), tensor([-0.1213,  0.5655, -0.2023,  ..., -0.6556, -0.0689,  0.2560]), tensor([-0.0823,  0.5951, -0.2356,  ..., -0.6488, -0.0407,  0.2196]), tensor([-0.0729,  0.5509, -0.2185,  ..., -0.6522, -0.0173,  0.2274]), tensor([-0.1574,  0.5029, -0.1943,  ..., -0.7982,  0.0262,  0.1938]), tensor([-0.1358,  0.5242, -0.2903,  ..., -0.7048, -0.0320,  0.2052]), tensor([-0.1405,  0.5391, -0.2918,  ..., -0.6326, -0.0607,  0.2593]), tensor([-0.1437,  0.5415, -0.3125,  ..., -0.6945, -0.0647,  0.2874]), tensor([-0.0985,  0.5276, -0.3087,  ..., -0.6899, -0.0666,  0.3265]), tensor([-0.1791,  0.5173, -0.2332,  ..., -0.7217, -0.0128,  0.2710]), tensor([-0.1864,  0.4944, -0.1953,  ..., -0.6737, -0.0373,  0.2763]), tensor([ 0.5343,  0.1720,  0.3373,  ..., -0.4143, -0.5902, -0.2041]), tensor([-0.1790,  0.5448, -0.1588,  ..., -0.6537, -0.0813,  0.2883]), tensor([-0.1751,  0.5736, -0.2132,  ..., -0.6319, -0.0433,  0.2584]), tensor([-0.1026,  0.5090, -0.1546,  ..., -0.5862, -0.0028,  0.1995]), tensor([-0.1236,  0.4727, -0.1475,  ..., -0.5262, -0.0255,  0.1840]), tensor([-0.1022,  0.4423, -0.2484,  ..., -0.5843, -0.0210,  0.1819]), tensor([-0.0906,  0.4803, -0.2942,  ..., -0.5452,  0.0647,  0.2049]), tensor([-0.2014,  0.2028, -0.0504,  ..., -0.2829, -0.0015,  0.0848]), tensor([-0.1001,  0.4471, -0.3344,  ..., -0.5140,  0.0389,  0.1865]), tensor([-0.1584,  0.1758,  0.0007,  ..., -0.3254, -0.0187,  0.0997]), tensor([-0.0741,  0.3838, -0.3672,  ..., -0.5070,  0.0292,  0.1980])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## Equinor\n",
    "\n",
    "with open(\"data/equinor_annual.pkl\",\"rb\") as file:\n",
    "    equinor_pdfs = pickle.load(file)\n",
    "\n",
    "print(equinor_pdfs.keys())\n",
    "\n",
    "equinor_annual = []\n",
    "for year in equinor_pdfs:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in equinor_pdfs[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    equinor_annual.append(mean)\n",
    "equinor_annual.reverse()\n",
    "\n",
    "print(f\"Equinor pdfs ({len(equinor_annual)}): {equinor_annual}\")  ## 1999-2023\n",
    "\n",
    "with open(\"equinor_annual_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(equinor_annual,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nyhetsartikler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exxon (9): [array([-0.24312055,  0.5051784 ,  0.1549819 , ..., -0.6929625 ,\n",
      "       -0.24416393,  0.20473239], dtype=float32), array([-0.29078484,  0.5914777 ,  0.13950814, ..., -0.7001572 ,\n",
      "       -0.25072357,  0.20023325], dtype=float32), array([-0.20187528,  0.6276899 ,  0.10301138, ..., -0.74484354,\n",
      "       -0.26047188,  0.18062863], dtype=float32), array([-0.24780482,  0.5994666 ,  0.07525771, ..., -0.7720047 ,\n",
      "       -0.2908926 ,  0.16564277], dtype=float32), array([-0.22474691,  0.5233269 , -0.01631049, ..., -0.7311823 ,\n",
      "       -0.29212835,  0.04337287], dtype=float32), array([-0.23074628,  0.52626956,  0.08668005, ..., -0.7644288 ,\n",
      "       -0.36406428,  0.02450266], dtype=float32), array([-0.26597995,  0.5513826 ,  0.03799932, ..., -0.81161195,\n",
      "       -0.4229434 ,  0.01807259], dtype=float32), array([-0.29241958,  0.46201125,  0.14774753, ..., -0.61416876,\n",
      "       -0.35401216,  0.04904832], dtype=float32), array([-0.24218343,  0.52749103,  0.07972469, ..., -0.6482674 ,\n",
      "       -0.38613802,  0.06073271], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "## Exxon\n",
    "\n",
    "with open('data/Exxon_news_embeddings.pkl', 'rb') as file:\n",
    "    exxon_news = pickle.load(file)\n",
    "\n",
    "exxon_news = snitt(exxon_news)\n",
    "print(f\"Exxon ({len(exxon_news)}): {exxon_news}\")  ## 2016-2024\n",
    "\n",
    "with open(\"exxon_news_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(exxon_news,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AkerBP news (9): [array([ 0.34267336,  0.58903885, -0.04372702, ..., -1.0807465 ,\n",
      "       -0.57759476,  0.2693308 ], dtype=float32), array([ 0.2573317 ,  0.39258513, -0.1900722 , ..., -1.1565386 ,\n",
      "       -0.5862084 ,  0.17565815], dtype=float32), array([ 0.21637367,  0.27790096, -0.1462012 , ..., -1.114886  ,\n",
      "       -0.70084953,  0.24041098], dtype=float32), array([ 0.3084579 ,  0.52573806, -0.23526993, ..., -1.1351568 ,\n",
      "       -0.4362622 ,  0.37928843], dtype=float32), array([ 0.3581431 ,  0.43783376, -0.2686381 , ..., -1.116644  ,\n",
      "       -0.544963  ,  0.21410924], dtype=float32), array([ 0.3094682 ,  0.54865134, -0.15052861, ..., -1.0879618 ,\n",
      "       -0.4746925 ,  0.24485879], dtype=float32), array([ 0.22245272,  0.4727047 , -0.35373348, ..., -0.89436203,\n",
      "       -0.38583446,  0.2845176 ], dtype=float32), array([ 0.3404096 ,  0.5370492 , -0.17731668, ..., -0.99929506,\n",
      "       -0.39034832,  0.22795507], dtype=float32), array([ 0.49007818,  0.63084036, -0.02246108, ..., -0.8651808 ,\n",
      "       -0.49745318,  0.38556156], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "## AkerBP\n",
    "\n",
    "with open('data/AkerBP_news_embeddings.pkl', 'rb') as file:\n",
    "    akerBP_news = pickle.load(file)\n",
    "\n",
    "akerBP_news = snitt(akerBP_news)\n",
    "print(f\"AkerBP news ({len(akerBP_news)}): {akerBP_news}\")  ## 2016-2024\n",
    "\n",
    "with open(\"akerBP_news_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(akerBP_news,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['2024', '2023', '2022', '2021', '2020', '2019', '2018', '2017'])\n",
      "Chevron (8): [tensor([-0.0993,  0.5528,  0.4099,  ..., -0.6097, -0.6342, -0.0937]), tensor([-0.1583,  0.6644,  0.4357,  ..., -0.4674, -0.3196, -0.3182]), tensor([-0.2072,  0.6092,  0.3804,  ..., -0.4745, -0.3737, -0.2600]), tensor([-0.1344,  0.5919,  0.4013,  ..., -0.4886, -0.2411, -0.2183]), tensor([-0.1425,  0.5735,  0.2986,  ..., -0.3629, -0.3245, -0.3766]), tensor([ 0.0298,  0.7221,  0.1533,  ..., -0.5573, -0.4069, -0.2374]), tensor([ 0.1201,  0.6498,  0.2372,  ..., -0.4784, -0.2591, -0.1406]), tensor([ 0.0853,  0.6971,  0.2261,  ..., -0.4286, -0.2507, -0.2756])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/2971005995.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in chevron_data[year]]\n",
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## Chevron\n",
    "\n",
    "with open(\"data/Chevron_news_embeddings.pkl\", \"rb\") as file:\n",
    "    chevron_data = pickle.load(file)\n",
    "\n",
    "print(chevron_data.keys())\n",
    "\n",
    "chevron_news = []\n",
    "for year in chevron_data:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in chevron_data[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    chevron_news.append(mean)\n",
    "chevron_news.reverse()\n",
    "print(f\"Chevron ({len(chevron_news)}): {chevron_news}\")  ## 2017-2024\n",
    "\n",
    "with open(\"chevron_news_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(chevron_news,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BP (15): [tensor([-0.1312,  0.4203, -0.0532,  ..., -0.5722, -0.4073,  0.3390]), tensor([-0.0524,  0.2815, -0.1385,  ..., -0.4713, -0.2989,  0.2424]), tensor([-0.1391,  0.3001, -0.0605,  ..., -0.4873, -0.2289,  0.2614]), tensor([-0.1556,  0.1242, -0.0238,  ..., -0.3749, -0.2597,  0.2490]), tensor([-0.1879,  0.1725, -0.0122,  ..., -0.3649, -0.3185,  0.2832]), tensor([-0.1635,  0.1255,  0.0358,  ..., -0.3924, -0.3174,  0.2107]), tensor([-0.1206,  0.2371,  0.0305,  ..., -0.4374, -0.3793,  0.1856]), tensor([-0.2209,  0.3601,  0.0490,  ..., -0.4100, -0.3660,  0.0480]), tensor([-0.1386,  0.3327,  0.0816,  ..., -0.4588, -0.4408,  0.1162]), tensor([-0.1222,  0.2862, -0.0777,  ..., -0.4748, -0.3873,  0.0974]), tensor([-0.1660,  0.2979, -0.0861,  ..., -0.5161, -0.4064,  0.0789]), tensor([-0.0903,  0.2799, -0.0976,  ..., -0.5006, -0.2960,  0.0269]), tensor([-0.0924,  0.2886, -0.1041,  ..., -0.4523, -0.4129, -0.0433]), tensor([-0.0537,  0.3087, -0.0863,  ..., -0.4596, -0.4204, -0.0397]), tensor([-0.1205,  0.2297, -0.1019,  ..., -0.4051, -0.2936, -0.0286])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1996544191.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in BP_data[year]]\n",
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## BP\n",
    "\n",
    "with open(\"data/BP_news_embeddings.pkl\",\"rb\") as file:\n",
    "    BP_data = pickle.load(file)\n",
    "\n",
    "BP_news = []\n",
    "for year in BP_data:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in BP_data[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    BP_news.append(mean)\n",
    "BP_news.reverse()\n",
    "print(f\"BP ({len(BP_news)}): {BP_news}\")  ## 2010-2024\n",
    "\n",
    "with open(\"BP_news_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(BP_news,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024'])\n",
      "Equinor news (27): [tensor([ 0.0069,  0.5014, -0.1360,  ..., -0.7030, -0.2611,  0.3027]), tensor([ 0.0693,  0.4917, -0.0764,  ..., -0.7579, -0.2302,  0.2760]), tensor([ 0.0293,  0.5073, -0.0462,  ..., -0.7679, -0.2541,  0.2723]), tensor([ 0.0405,  0.4672, -0.0935,  ..., -0.7549, -0.1758,  0.2891]), tensor([ 0.0523,  0.4806,  0.0147,  ..., -0.6645, -0.2273,  0.3181]), tensor([ 0.0997,  0.4316,  0.0012,  ..., -0.7622, -0.2855,  0.3526]), tensor([ 0.0386,  0.5925, -0.1093,  ..., -0.8265, -0.2617,  0.3212]), tensor([ 0.0207,  0.5362, -0.0831,  ..., -0.7259, -0.1994,  0.3137]), tensor([ 0.0900,  0.4761, -0.0175,  ..., -0.6810, -0.2584,  0.2394]), tensor([-0.0685,  0.5817, -0.0278,  ..., -0.7223, -0.1448,  0.1565]), tensor([-0.0565,  0.5099, -0.0510,  ..., -0.7410, -0.2875,  0.2076]), tensor([ 0.0500,  0.5965, -0.0700,  ..., -0.7126, -0.2565,  0.1990]), tensor([ 0.0378,  0.3931,  0.0185,  ..., -0.5183, -0.2368,  0.1943]), tensor([ 0.1322,  0.5245, -0.0475,  ..., -0.6191, -0.2101,  0.2808]), tensor([ 0.0467,  0.5298, -0.1000,  ..., -0.7457, -0.1508,  0.3709]), tensor([ 0.0402,  0.4931, -0.0600,  ..., -0.7135, -0.2578,  0.3971]), tensor([ 0.0717,  0.5474, -0.1212,  ..., -0.6778, -0.2555,  0.3515]), tensor([ 0.0507,  0.3789, -0.1089,  ..., -0.6709, -0.2509,  0.3594]), tensor([ 0.0375,  0.4942, -0.1246,  ..., -0.5631, -0.3375,  0.3322]), tensor([-0.0023,  0.6345, -0.1348,  ..., -0.5339, -0.3420,  0.3557]), tensor([ 0.2491,  0.4282, -0.0961,  ..., -0.5898, -0.1641,  0.1990]), tensor([ 0.2770,  0.4426, -0.0273,  ..., -0.5320, -0.1878,  0.1584]), tensor([ 0.2101,  0.4108, -0.1277,  ..., -0.5859, -0.1338,  0.1496]), tensor([ 0.2047,  0.4598, -0.1277,  ..., -0.4527, -0.0629,  0.1699]), tensor([ 0.1251,  0.5317, -0.2419,  ..., -0.6033, -0.1387,  0.1796]), tensor([ 0.3067,  0.6013, -0.0947,  ..., -0.4807, -0.2169,  0.2007]), tensor([ 0.2312,  0.4303, -0.2479,  ..., -0.4906, -0.1420,  0.0488])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/497622995.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in equinor_data[year]]\n",
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## Equinor\n",
    "\n",
    "with open(\"data/equinor_news_embeddings.pkl\",\"rb\") as file:\n",
    "    equinor_data = pickle.load(file)\n",
    "\n",
    "print(equinor_data.keys())\n",
    "\n",
    "equinor_news = []\n",
    "for year in equinor_data:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in equinor_data[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    equinor_news.append(mean)\n",
    "\n",
    "print(f\"Equinor news ({len(equinor_news)}): {equinor_news}\")  ## 1998-2024\n",
    "\n",
    "with open(\"equinor_news_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(equinor_news,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['2020', '2021', '2022', '2023', '2024'])\n",
      "Equinor news (5): [tensor([ 0.1218,  0.2564, -0.3867,  ..., -0.6963, -0.4214, -0.1180]), tensor([ 0.2313,  0.3329, -0.3187,  ..., -0.5984, -0.3937, -0.1454]), tensor([ 0.0870,  0.3290, -0.3141,  ..., -0.7825, -0.5733, -0.1110]), tensor([ 0.2535,  0.3405, -0.3742,  ..., -0.8614, -0.4780, -0.0655]), tensor([ 0.0126,  0.2183, -0.2820,  ..., -0.7747, -0.5002, -0.0255])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## PetroChina\n",
    "\n",
    "with open(\"data/PetroChina_newsarticles.pkl\",\"rb\") as file:\n",
    "    petroChina_data = pickle.load(file)\n",
    "\n",
    "print(petroChina_data.keys())\n",
    "\n",
    "petroChina_news = []\n",
    "for year in petroChina_data:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in petroChina_data[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    petroChina_news.append(mean)\n",
    "\n",
    "print(f\"Equinor news ({len(petroChina_news)}): {petroChina_news}\")  ## 1998-2024\n",
    "\n",
    "with open(\"petroChina_news_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(petroChina_news,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024])\n",
      "CNOOC news (22): [tensor([-0.0776,  0.0398, -0.0066,  ..., -1.0461, -0.3627, -0.1458]), tensor([-0.0063,  0.0137,  0.0168,  ..., -0.9499, -0.3949, -0.1371]), tensor([-0.0706, -0.2282, -0.0376,  ..., -0.5949, -0.2473, -0.1513]), tensor([-0.2002, -0.7236,  0.2077,  ..., -0.1646, -0.2123, -0.3361]), tensor([-0.2197, -0.3770,  0.0764,  ..., -0.6046, -0.1756, -0.2683]), tensor([-0.2383, -0.6469,  0.1561,  ..., -0.1730, -0.1214, -0.2721]), tensor([-0.1984,  0.1003, -0.0135,  ..., -0.7864, -0.4714, -0.1995]), tensor([-0.1094,  0.0977, -0.0797,  ..., -0.9606, -0.3849, -0.0914]), tensor([-0.1189, -0.0726, -0.0957,  ..., -0.7890, -0.3297, -0.0405]), tensor([-0.1576,  0.1190, -0.0779,  ..., -0.9874, -0.2673, -0.0483]), tensor([-0.3064, -0.1588,  0.0356,  ..., -0.6992, -0.3987, -0.2963]), tensor([-0.1554,  0.1227, -0.0380,  ..., -0.9708, -0.3866, -0.0361]), tensor([-0.0483, -0.0722, -0.0476,  ..., -1.0839, -0.4809,  0.0254]), tensor([-0.2525, -0.2441, -0.1528,  ..., -1.0915, -0.4971,  0.0084]), tensor([-0.1838,  0.1186, -0.1596,  ..., -1.0918, -0.3951, -0.0137]), tensor([-0.1631, -0.0944,  0.0862,  ..., -1.0412, -0.4557, -0.0602]), tensor([-0.0591, -0.1195, -0.0928,  ..., -1.0908, -0.4996, -0.0314]), tensor([-0.2078, -0.0794, -0.2567,  ..., -1.0703, -0.5076, -0.1207]), tensor([-0.2574, -0.1076, -0.0836,  ..., -0.7843, -0.5675, -0.1371]), tensor([-0.1990,  0.0397, -0.1562,  ..., -1.0950, -0.5775, -0.0855]), tensor([-0.2232,  0.0798, -0.1871,  ..., -1.0504, -0.5447, -0.0067]), tensor([-0.1500,  0.0342, -0.1984,  ..., -1.2115, -0.5804, -0.0127])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1973638030.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in CNOOC_data[year]]\n",
      "/var/folders/jj/d77tn1s96j9dgz5sx1cf0b500000gn/T/ipykernel_64898/1462058501.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_embeddings = [torch.tensor(emb) for emb in embeddings]\n"
     ]
    }
   ],
   "source": [
    "## CNOOC\n",
    "\n",
    "with open(\"data/CNOOC_news_embeddings.pkl\",\"rb\") as file:\n",
    "    CNOOC_data = pickle.load(file)\n",
    "\n",
    "print(CNOOC_data.keys())\n",
    "\n",
    "CNOOC_news = []\n",
    "for year in CNOOC_data:\n",
    "    tensor_embeddings = [torch.tensor(emb) for emb in CNOOC_data[year]]\n",
    "    mean = snitt2(tensor_embeddings)\n",
    "    CNOOC_news.append(mean)\n",
    "\n",
    "print(f\"CNOOC news ({len(CNOOC_news)}): {CNOOC_news}\")  ## 1998-2024\n",
    "\n",
    "with open(\"CNOOC_news_snitt.pkl\",\"wb\") as file:\n",
    "    pickle.dump(CNOOC_news,file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
